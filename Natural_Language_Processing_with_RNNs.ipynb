{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing with RNNs.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRcfD4wqQcpb1b2iLyDVur",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GauraoM/Tensorflow/blob/main/Natural_Language_Processing_with_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bag of words\n",
        " It does not maintain the order of the words but does keep track of the frequency."
      ],
      "metadata": {
        "id": "G_A8OD2RPrlk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpG05iwNOai9",
        "outputId": "659956ce-4251-49af-ac57-a3d76d91fa91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ]
        }
      ],
      "source": [
        "vocab = {}  # maps word to integer representing it\n",
        "word_encoding = 1\n",
        "def bag_of_words(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \")  # create a list of all of the words in the text\n",
        "  bag = {}  # stores all of the encodings and their frequency\n",
        "\n",
        "  # Iterate over words\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      encoding = vocab[word]  # get encoding from vocab\n",
        "    else:\n",
        "      vocab[word] = word_encoding # Assign word_encodeing to the word\n",
        "      encoding = word_encoding \n",
        "      word_encoding += 1\n",
        "    \n",
        "    if encoding in bag:\n",
        "      bag[encoding] += 1\n",
        "    else:\n",
        "      bag[encoding] = 1\n",
        "  \n",
        "  return bag\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "bag = bag_of_words(text)\n",
        "print(bag)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IMDB Movie Review Dataset\n",
        "This dataset contains 25,000 reviews from IMDB where each one is already preprocessed and has a label as either positive or negative."
      ],
      "metadata": {
        "id": "h967wDxgStoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#load the data\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1unqiXjlRTFQ",
        "outputId": "93e09ebb-aff4-44f1-c977-e510290cdb87"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check one review\n",
        "train_data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cl6c82yTb1l",
        "outputId": "1dd0a099-880c-4dc8-a714-be7bcf5fc170"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 194,\n",
              " 1153,\n",
              " 194,\n",
              " 8255,\n",
              " 78,\n",
              " 228,\n",
              " 5,\n",
              " 6,\n",
              " 1463,\n",
              " 4369,\n",
              " 5012,\n",
              " 134,\n",
              " 26,\n",
              " 4,\n",
              " 715,\n",
              " 8,\n",
              " 118,\n",
              " 1634,\n",
              " 14,\n",
              " 394,\n",
              " 20,\n",
              " 13,\n",
              " 119,\n",
              " 954,\n",
              " 189,\n",
              " 102,\n",
              " 5,\n",
              " 207,\n",
              " 110,\n",
              " 3103,\n",
              " 21,\n",
              " 14,\n",
              " 69,\n",
              " 188,\n",
              " 8,\n",
              " 30,\n",
              " 23,\n",
              " 7,\n",
              " 4,\n",
              " 249,\n",
              " 126,\n",
              " 93,\n",
              " 4,\n",
              " 114,\n",
              " 9,\n",
              " 2300,\n",
              " 1523,\n",
              " 5,\n",
              " 647,\n",
              " 4,\n",
              " 116,\n",
              " 9,\n",
              " 35,\n",
              " 8163,\n",
              " 4,\n",
              " 229,\n",
              " 9,\n",
              " 340,\n",
              " 1322,\n",
              " 4,\n",
              " 118,\n",
              " 9,\n",
              " 4,\n",
              " 130,\n",
              " 4901,\n",
              " 19,\n",
              " 4,\n",
              " 1002,\n",
              " 5,\n",
              " 89,\n",
              " 29,\n",
              " 952,\n",
              " 46,\n",
              " 37,\n",
              " 4,\n",
              " 455,\n",
              " 9,\n",
              " 45,\n",
              " 43,\n",
              " 38,\n",
              " 1543,\n",
              " 1905,\n",
              " 398,\n",
              " 4,\n",
              " 1649,\n",
              " 26,\n",
              " 6853,\n",
              " 5,\n",
              " 163,\n",
              " 11,\n",
              " 3215,\n",
              " 10156,\n",
              " 4,\n",
              " 1153,\n",
              " 9,\n",
              " 194,\n",
              " 775,\n",
              " 7,\n",
              " 8255,\n",
              " 11596,\n",
              " 349,\n",
              " 2637,\n",
              " 148,\n",
              " 605,\n",
              " 15358,\n",
              " 8003,\n",
              " 15,\n",
              " 123,\n",
              " 125,\n",
              " 68,\n",
              " 23141,\n",
              " 6853,\n",
              " 15,\n",
              " 349,\n",
              " 165,\n",
              " 4362,\n",
              " 98,\n",
              " 5,\n",
              " 4,\n",
              " 228,\n",
              " 9,\n",
              " 43,\n",
              " 36893,\n",
              " 1157,\n",
              " 15,\n",
              " 299,\n",
              " 120,\n",
              " 5,\n",
              " 120,\n",
              " 174,\n",
              " 11,\n",
              " 220,\n",
              " 175,\n",
              " 136,\n",
              " 50,\n",
              " 9,\n",
              " 4373,\n",
              " 228,\n",
              " 8255,\n",
              " 5,\n",
              " 25249,\n",
              " 656,\n",
              " 245,\n",
              " 2350,\n",
              " 5,\n",
              " 4,\n",
              " 9837,\n",
              " 131,\n",
              " 152,\n",
              " 491,\n",
              " 18,\n",
              " 46151,\n",
              " 32,\n",
              " 7464,\n",
              " 1212,\n",
              " 14,\n",
              " 9,\n",
              " 6,\n",
              " 371,\n",
              " 78,\n",
              " 22,\n",
              " 625,\n",
              " 64,\n",
              " 1382,\n",
              " 9,\n",
              " 8,\n",
              " 168,\n",
              " 145,\n",
              " 23,\n",
              " 4,\n",
              " 1690,\n",
              " 15,\n",
              " 16,\n",
              " 4,\n",
              " 1355,\n",
              " 5,\n",
              " 28,\n",
              " 6,\n",
              " 52,\n",
              " 154,\n",
              " 462,\n",
              " 33,\n",
              " 89,\n",
              " 78,\n",
              " 285,\n",
              " 16,\n",
              " 145,\n",
              " 95]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing\n",
        "As each word is of different length we can't proceed with that so se havs to do some padding"
      ],
      "metadata": {
        "id": "zag4iThMUOUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)"
      ],
      "metadata": {
        "id": "EE3EiX4vTpZS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating the model"
      ],
      "metadata": {
        "id": "Sz7cqxQNVnok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32), # 32 is the output dimenssion of the vector generated by embeddings\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "7hJc5EZtU48S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNmnzxUuXaah",
        "outputId": "9f0b0b8b-2658-49f8-be2a-bfad1dcca6b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          2834688   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                8320      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the model"
      ],
      "metadata": {
        "id": "wSNwG-qfXsmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
        " # Train the model with the validation split of 20%\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk5FC0dSXp_P",
        "outputId": "e23ef815-2189-422e-8d08-13ca9a014a26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 42s 56ms/step - loss: 0.4395 - acc: 0.8033 - val_loss: 0.2967 - val_acc: 0.8808\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 34s 55ms/step - loss: 0.2441 - acc: 0.9068 - val_loss: 0.2908 - val_acc: 0.8846\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.1887 - acc: 0.9316 - val_loss: 0.2738 - val_acc: 0.8850\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.1553 - acc: 0.9439 - val_loss: 0.3091 - val_acc: 0.8898\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.1312 - acc: 0.9539 - val_loss: 0.3335 - val_acc: 0.8812\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 35s 57ms/step - loss: 0.1106 - acc: 0.9609 - val_loss: 0.3123 - val_acc: 0.8860\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.0984 - acc: 0.9657 - val_loss: 0.3183 - val_acc: 0.8838\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 35s 57ms/step - loss: 0.0878 - acc: 0.9710 - val_loss: 0.4054 - val_acc: 0.8744\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 35s 57ms/step - loss: 0.0774 - acc: 0.9754 - val_loss: 0.5163 - val_acc: 0.8716\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.0690 - acc: 0.9776 - val_loss: 0.3874 - val_acc: 0.8798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate the performance"
      ],
      "metadata": {
        "id": "oYouwzNybGHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92VqbXULa_xa",
        "outputId": "e7dd5180-d112-4f5d-ca04-5d5b8b1ca669"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 13s 16ms/step - loss: 0.4774 - acc: 0.8574\n",
            "[0.47738516330718994, 0.8573600053787231]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Making predictions\n",
        "Since reviews are encoded we will need to convert any review that we write into that form so the network can understand it. \n",
        "To do that well load the encodings from the dataset and use them to encode our own data."
      ],
      "metadata": {
        "id": "eSMCiiQScHjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the encodeings\n",
        "word_index= imdb.get_word_index()\n",
        "\n",
        "# function to encode the text\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text) # Converted the word into token\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens] # assign word index if it present else return 0\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC7vrHCzcp-a",
        "outputId": "988602b5-8a51-4044-b812-eb99edb959c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoding the text\n",
        "\n",
        "reverse_word_index = {value: key for (key,value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "  PAD = 0\n",
        "  text = \"\"\n",
        "  # Iterating over integer \n",
        "  for num in integers:\n",
        "    if num != PAD: # If it is not zero \n",
        "       text += reverse_word_index[num] + \" \"\n",
        "\n",
        "  return text[:-1] \n",
        "\n",
        "print(decode_integers(encoded))      \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C84iDU2eQNT",
        "outputId": "baa5d435-d904-4c9c-c0bb-4e8442912bc8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "that movie was just amazing so amazing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now time to make a prediction\n",
        "\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text) # Encode the text\n",
        "  pred = np.zeros((1,250)) \n",
        "  pred[0] = encoded_text # Insert the encoded text\n",
        "  result = model.predict(pred) \n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz4moWlHhtn9",
        "outputId": "7544eca1-fb1b-4bfc-bbe3-ad99b6ac23b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.960262]\n",
            "[0.6202364]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN play generator\n",
        "Hers we will simply show the RNN an example of something we want it to recreate and it will learn how to write a version of it on its own"
      ],
      "metadata": {
        "id": "atxcr-P-jNR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "runsHqnpjd-J"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dataset\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "XJyyCiOVjh-V"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the content of file\n",
        "\n",
        "# Read, then decode\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_xkZ_nXj084",
        "outputId": "236e526e-e6da-4e4d-fdfb-700bc7c14693"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUxdDJE2lBIh",
        "outputId": "995076c5-8bf9-4c8b-e06a-3125fee169d4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text)) #get unique characters in a vocabulory\n",
        "\n",
        "# Mapping the unique characters to indices\n",
        "char2idx = {u:i for i,u in enumerate(vocab)} # get the indices for letter\n",
        "print(char2idx)\n",
        "idx2char = np.array(vocab) # Convert to an array so we can just use index at which a letter appears\n",
        "print(idx2char)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQTej0QLlFxV",
        "outputId": "da323d60-4c23-4356-c13f-61454163b5c6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
            "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
            " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
            " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
            " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cymzep99nCWk",
        "outputId": "67752ae5-b36d-4ee2-a3e6-bc15bffbae3b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "    #print(ints)\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u4KtCZ0nybw",
        "outputId": "2ffd7d1b-ac27-469d-b6bb-b9a1732f2321"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating training example"
      ],
      "metadata": {
        "id": "BZqBug4-opYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1) #as we require 100 exaples per epoch\n",
        "\n",
        "# Create training examples / targets\n",
        "# convert entire string of data into characters\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "z5gARd7IoP-e"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the stream of characters into batch of desired length\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "zGti8k2Oqgl7"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target) # apply to every entry above"
      ],
      "metadata": {
        "id": "dW_ZiVf6rX2U"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Make Training batches "
      ],
      "metadata": {
        "id": "ruwsbFn1svqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "uhhlSs6Brv9y"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build the model"
      ],
      "metadata": {
        "id": "huskwcBEuru_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY1HwFrruuO8",
        "outputId": "3c1f142a-6212-427e-e6bd-ddc889216d3a"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating Loss Function"
      ],
      "metadata": {
        "id": "i1IylSoIx5mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at sample input and output for the untrained model\n",
        "\n",
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \": (batch_size, sequence_length, vocab_size)\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI0wK53DwKHA",
        "outputId": "8bb42c58-43af-4d1d-ad78-79524d81db36"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) : (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2ucangsxx3f",
        "outputId": "56ea5a99-3fac-4d56-839f-b485422e157a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-3.83939850e-03  3.58762394e-04 -1.93279632e-03 ... -2.76192511e-03\n",
            "   -2.69576744e-03 -8.59825930e-04]\n",
            "  [-6.21288968e-03 -4.17579402e-04 -4.10450110e-03 ... -4.35518287e-03\n",
            "   -4.66823531e-03 -2.61309184e-03]\n",
            "  [ 2.43651331e-03 -2.34970776e-03 -2.72694830e-04 ...  5.72698307e-04\n",
            "   -3.08301486e-03 -3.37857124e-03]\n",
            "  ...\n",
            "  [-4.54373937e-03 -1.20143257e-02  4.02230391e-04 ... -1.09200692e-03\n",
            "   -1.37980573e-03  1.17104976e-02]\n",
            "  [-9.58475284e-04 -3.45858093e-03 -1.21976202e-03 ...  2.44349777e-03\n",
            "   -3.06397630e-03  1.20440405e-02]\n",
            "  [-5.10210264e-03 -2.17005261e-03 -5.84709598e-03 ...  9.00705904e-03\n",
            "    4.59635118e-03  8.84665363e-03]]\n",
            "\n",
            " [[-1.47111085e-03 -2.14975653e-03 -5.36827696e-03 ...  2.00513913e-03\n",
            "    1.27182459e-03  5.73710725e-03]\n",
            "  [ 2.56006746e-03 -3.26226000e-03 -4.34269151e-03 ...  2.21574563e-03\n",
            "    6.30671624e-03  1.36583834e-03]\n",
            "  [ 6.32131193e-03  6.04782312e-04 -7.64064584e-03 ...  4.14719619e-03\n",
            "    6.66456670e-03 -5.53358486e-03]\n",
            "  ...\n",
            "  [ 2.34875851e-03  7.86011759e-03  7.28192553e-03 ...  3.43918893e-03\n",
            "    1.23050632e-02  6.52118959e-03]\n",
            "  [-2.58884463e-03  6.63560955e-03  4.41875122e-03 ...  1.02063594e-03\n",
            "    6.61622221e-03  3.97028681e-03]\n",
            "  [ 2.19561101e-04  4.73500928e-03  4.19565290e-03 ...  9.02528118e-04\n",
            "    5.33592328e-03  8.13176669e-03]]\n",
            "\n",
            " [[-2.80297943e-03  2.71628000e-04  3.33518046e-03 ...  2.84138671e-03\n",
            "   -5.75992744e-04  3.72757996e-03]\n",
            "  [ 1.59827946e-03 -2.68535246e-03  2.85002845e-03 ...  7.51885772e-03\n",
            "   -2.79424177e-03  5.42625086e-03]\n",
            "  [-5.60798973e-04 -3.90298339e-03 -3.25711421e-03 ...  8.23710300e-03\n",
            "   -6.30885595e-04  1.03946496e-02]\n",
            "  ...\n",
            "  [-3.23761022e-03  1.89535983e-03  3.40312556e-03 ...  1.79813001e-02\n",
            "    5.09195216e-03  5.22062276e-03]\n",
            "  [-4.89672413e-03  4.82498232e-04 -1.55981036e-03 ...  1.57691203e-02\n",
            "    4.81070997e-03  9.48247500e-03]\n",
            "  [-5.70180034e-03  1.96944224e-03  9.45251668e-04 ...  9.59672127e-03\n",
            "    3.34701664e-03  1.00798197e-02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 2.24818289e-03  3.73503688e-04  7.50029576e-04 ... -1.16666011e-03\n",
            "    5.93110221e-04  6.46984298e-03]\n",
            "  [ 6.45536790e-03  6.14913646e-03  4.39834548e-04 ... -1.13080535e-03\n",
            "    9.86941392e-04  1.32220788e-02]\n",
            "  [ 4.83973417e-03  8.52374081e-03  2.74138083e-03 ... -8.03904783e-04\n",
            "   -5.82325086e-03  1.32755032e-02]\n",
            "  ...\n",
            "  [-3.56387999e-03  7.49538140e-03  3.04836268e-03 ...  1.14663620e-03\n",
            "    6.22708257e-03  3.22080427e-03]\n",
            "  [ 3.85913881e-04  4.88367956e-03  2.42753490e-03 ...  2.99384375e-03\n",
            "    9.17758141e-03 -2.75901548e-05]\n",
            "  [-2.62056332e-04 -1.25851412e-03 -3.11525748e-03 ... -6.10410410e-04\n",
            "    1.23205716e-02  9.51632764e-03]]\n",
            "\n",
            " [[-5.08703059e-04  3.47106997e-03  2.71402439e-03 ... -1.21905305e-03\n",
            "    4.85364813e-03 -3.13370582e-03]\n",
            "  [ 5.79039473e-03  2.72654975e-03  5.48824482e-03 ...  1.77860062e-03\n",
            "    3.96395847e-03 -3.02949362e-03]\n",
            "  [ 6.10329118e-03  3.31341079e-03  4.13580751e-03 ...  4.71192034e-04\n",
            "    3.59004224e-03  2.74066138e-03]\n",
            "  ...\n",
            "  [ 3.56991892e-03  1.09549507e-03 -3.44829145e-03 ...  9.55688185e-04\n",
            "    2.38846848e-03  9.03525297e-03]\n",
            "  [ 4.14268952e-03 -2.09126651e-04 -4.43389639e-03 ...  3.11636971e-03\n",
            "    6.41947892e-03  5.01917768e-03]\n",
            "  [-2.43638433e-03  1.47931496e-04 -7.66893523e-03 ...  9.36007779e-03\n",
            "    1.21420845e-02  2.34822556e-03]]\n",
            "\n",
            " [[-3.86732654e-03  1.12424226e-04 -3.47905443e-03 ...  7.49025960e-03\n",
            "    6.99633500e-03 -1.79246257e-04]\n",
            "  [-5.62781584e-04 -7.63159303e-04 -4.24980419e-03 ...  6.39983639e-03\n",
            "    9.49828979e-03  1.96006207e-04]\n",
            "  [ 1.11441116e-03  1.21307517e-06 -3.14246234e-03 ...  2.03212490e-03\n",
            "    7.60339480e-03  7.18188938e-03]\n",
            "  ...\n",
            "  [-9.01638903e-03  5.80590218e-03 -1.60911668e-03 ...  4.16014297e-03\n",
            "    1.12151299e-02  1.33995141e-03]\n",
            "  [-8.32562335e-03  4.60069720e-03 -3.07975709e-03 ...  1.61916425e-03\n",
            "    1.35834422e-03 -4.28124797e-03]\n",
            "  [-7.10718334e-03  6.89498102e-03  2.25562282e-04 ...  5.20798727e-04\n",
            "   -4.70834086e-03  2.13647101e-04]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "# this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXe4Wd0lx3U0",
        "outputId": "b68fc23b-361b-402b-f41a-015be8713dc3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-0.0038394   0.00035876 -0.0019328  ... -0.00276193 -0.00269577\n",
            "  -0.00085983]\n",
            " [-0.00621289 -0.00041758 -0.0041045  ... -0.00435518 -0.00466824\n",
            "  -0.00261309]\n",
            " [ 0.00243651 -0.00234971 -0.00027269 ...  0.0005727  -0.00308301\n",
            "  -0.00337857]\n",
            " ...\n",
            " [-0.00454374 -0.01201433  0.00040223 ... -0.00109201 -0.00137981\n",
            "   0.0117105 ]\n",
            " [-0.00095848 -0.00345858 -0.00121976 ...  0.0024435  -0.00306398\n",
            "   0.01204404]\n",
            " [-0.0051021  -0.00217005 -0.0058471  ...  0.00900706  0.00459635\n",
            "   0.00884665]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finally we look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred) #65 values representing the probabillity of each character occuring next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPzErNnwzCda",
        "outputId": "c329cf9a-ccba-49aa-932b-1271255defba"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-0.0038394   0.00035876 -0.0019328   0.00399611 -0.00434391 -0.00574383\n",
            " -0.0024333   0.00377613 -0.00123772 -0.00090148  0.00364577  0.00380444\n",
            " -0.00220031 -0.00024531 -0.00037837  0.00301075  0.00375164 -0.00179855\n",
            "  0.00521792 -0.00226365 -0.0043562  -0.00146387  0.00093601  0.00300207\n",
            " -0.00033476 -0.00375416  0.00023324  0.00144241  0.00677537 -0.00552182\n",
            "  0.00246178 -0.00086393 -0.00212277 -0.00114848 -0.00132068  0.00303585\n",
            " -0.00155842  0.00346775 -0.00164634  0.00100648 -0.00131003 -0.00079564\n",
            "  0.00129408 -0.00457165  0.00111241  0.00406929  0.00298475 -0.00017149\n",
            " -0.00220009 -0.00133352 -0.00031319 -0.0003282   0.00836229  0.00095998\n",
            "  0.00649619 -0.00046698 -0.00149029 -0.00011604  0.00488313 -0.00173706\n",
            "  0.00288528 -0.00386057 -0.00276193 -0.00269577 -0.00085983], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "WD_pARDRzb8N"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "pxKyoTHt0Khe"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "uBT2JELZ0VR3"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=20, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT5oeAda05_Y",
        "outputId": "6105c0a8-cae0-4b7e-d5a8-b36b0ab05343"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 31s 161ms/step - loss: 2.5573\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 1.8574\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 1.6165\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 1.4893\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 1.4120\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 1.3559\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 1.3116\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 1.2731\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 1.2364\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 1.2006\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 1.1635\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 1.1273\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 1.0882\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 1.0486\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 1.0079\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 0.9660\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 0.9246\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 0.8828\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 0.8448\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 0.8076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the model\n",
        "rebuild the model from a checkpoint using a batch_size of 1 so that we can feed one peice of text to the model and have it make a prediction."
      ],
      "metadata": {
        "id": "0yx_oA3GGJX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "0E5uRluCGM6P"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the weights\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "Sm9eadX4J7b9"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0) #[[]]\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        " \n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0) #convert [[]] to []\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "1rPnm4GLCEpM"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwRbRsaXCHXl",
        "outputId": "32ead0b5-03da-430e-8877-7d94ab4d48d8"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: romeo\n",
            "romeother honour,\n",
            "Say you that some warrant with this kneel\n",
            "Scoppary help there. -cause me not too much;\n",
            "If you take order,\n",
            "That he will raise her toman and twenty steed\n",
            "Through disting'd the noon-thou for your countrymand.\n",
            "\n",
            "YORK:\n",
            "Shall we to Choid upon your housey--and foothus to\n",
            "your command. What is the bed by the botton jury out,\n",
            "And still their fearful arms in Death.\n",
            "\n",
            "KING RICHARD II:\n",
            "O Bulourelord gops himself into grief:\n",
            "Now I must resolve me to thee! aras calm, come both.\n",
            "\n",
            "CATESBY:\n",
            "I'st it good, more:\n",
            "They shall yet dead a\n",
            "Florious shoulder mine: I did se out\n",
            "And throws what bad, what, and kiss you in mine ears,\n",
            "His glassy counsel, lords, with whom and clear\n",
            "Makes me not pitizen:\n",
            "And, for the morning pewders did exel him.\n",
            "First Warwick, this is upon her maid-paleaple.\n",
            "\n",
            "PETRUCHIO:\n",
            "What?\n",
            "\n"
          ]
        }
      ]
    }
  ]
}